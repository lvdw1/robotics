{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Furuta Pendulum\n",
    "\n",
    "This notebook provides a starting point for the project, introducing the simulation environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoreload extension to automatically reload modules when they are modified\n",
    "%load_ext autoreload\n",
    "\n",
    "# Set autoreload to reload all modules before executing code\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Dependencies\n",
    "\n",
    "The simulation and reinforcement learning algorithms used in this project require a few libraries. To install all the necessary dependencies, follow these steps:\n",
    "\n",
    "1. Open a terminal.\n",
    "2. Navigate to this directory.\n",
    "3. Run the following commands (assuming you have already set up the `conda` environment from the exercise sessions):\n",
    "\n",
    "```\n",
    "conda activate py13roboticscourse\n",
    "python -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Installing pytorch for Windows may require additional steps. If you encounter any issues, please refer to the official pytorch installation guide. Alternatively, you can work in a windows subsystem for Linux (WSL) environment, which is recommended for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pendulum configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "import os\n",
    "\n",
    "with open('pendulum_description/simulation_pendulum.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "parameters_model = config[\"parameters_model\"]\n",
    "urdf_path = os.path.join(\"pendulum_description\", config[\"urdf_filename\"])\n",
    "forward_dynamics_casadi_path = os.path.join(\"pendulum_description\", config[\"forward_dynamics_casadi_filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from furuta_torque_env import FurutaPendulumTorqueEnv\n",
    "\n",
    "gym.register(\n",
    "    id=\"FurutaPendulumTorque-v0\",\n",
    "    entry_point=FurutaPendulumTorqueEnv,\n",
    ")\n",
    "\n",
    "# Create an instance of the FurutaPendulumTorqueEnv environment\n",
    "env = gym.make(\"FurutaPendulumTorque-v0\",\n",
    "               urdf_model_path=urdf_path, forward_dynamics_casadi_path=forward_dynamics_casadi_path, parameters_model=parameters_model, render=True, swingup=True)\n",
    "\n",
    "# Click on the url that appears in the output to visualize the environment in a browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute some control actions on the environment\n",
    "\n",
    "Random actions are sampled and executed on the pendulum. In the browser visualization, you should see the pendulum moving, until the episode terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment to its initial state\n",
    "observation, info = env.reset()\n",
    "\n",
    "terminated = False\n",
    "truncated = False\n",
    "print(f\"Initial Observation: {observation}\")\n",
    "# Apply a sequence of actions\n",
    "while not terminated and not truncated:\n",
    "    action = env.action_space.sample()  # Sample a random action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Observation: {observation}, Reward: {reward}, Terminated: {terminated}, Truncated: {truncated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model and test it\n",
    "We provide a model of an agent that was already been trained to perform upward stabilization of the pendulum. \n",
    "\n",
    "\n",
    "This model was trained for 1 million timesteps using a slightly modified version of the [CleanRL implementation of PPO for continuous actions](https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy). You can find the script used for training (`ppo_continuous_action.py`) in this same directory.\n",
    "In the following cells, we demonstrate how to load the trained model and visualize its behavior in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ppo_continuous_action import Agent, make_env\n",
    "\n",
    "saved_model_path = 'furuta_pendulum_tensorboard/furutaTorque__ppo_continuous_action__42__1744890705/ppo_continuous_action.cleanrl_model' # no swingup\n",
    "\n",
    "# Set to False if task includes swingup\n",
    "swingup = False\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(urdf_path=urdf_path, \n",
    "                  parameters_model=parameters_model, \n",
    "                  forward_dynamics_casadi_path=forward_dynamics_casadi_path,\n",
    "                  render=True, swingup=swingup) for _ in range(1)]\n",
    "    )\n",
    "\n",
    "model = Agent(envs=envs)\n",
    "\n",
    "model.load_state_dict(torch.load(saved_model_path, map_location=\"cpu\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "observation, info = envs.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "print(f\"Initial Observation: {observation}\")\n",
    "# Apply a sequence of actions\n",
    "while not terminated and not truncated:\n",
    "    action, _, _, _ = model.get_action_and_value(torch.Tensor(observation), deterministic=True)  # Use the trained model to predict the action\n",
    "    action = action.cpu().detach().numpy()\n",
    "    observation, reward, terminated, truncated, info = envs.step(action)\n",
    "    print(f\"Observation: {observation}, Action: {action}, Reward: {reward}, Terminated: {terminated}, Truncated: {truncated}\")\n",
    "# Close the environment\n",
    "envs.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "Below you can see the charts of the training evolution, visualized through Tensorboard. These charts are generated automatically when training a model using the `ppo_continuos_action.py` script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir furuta_pendulum_tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
